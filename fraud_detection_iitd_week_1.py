# -*- coding: utf-8 -*-
"""fraud detection IITD week 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dW65af40W8TQZ6hBXCbUZjm2xjoLM2DM

**load csv file**
"""

import pandas as pd

df1=pd.read_csv("/content/transactions.gz")

"""**explore data**"""

df1.head()

df1.info()

"""**data cleaning**"""

df1.isnull().sum()

df1.shape

df1['isFraud'].value_counts()

fraud_data = df1[df1['isFraud'] == True]  # Filter for isFraud == True
null_in_fraud_rows = fraud_data.isnull().any(axis=1)  # Check for nulls in those rows
print(null_in_fraud_rows.any())  # See if any rows have nulls
print(null_in_fraud_rows.value_counts())

df1['acqCountry']=df1['acqCountry'].fillna('unknown')

df1['merchantCountryCode']=df1['merchantCountryCode'].fillna('unknown')
df1['merchantCountryCode'].isnull().sum()

df1['posEntryMode'].head(30)
df1['posEntryMode']=df1['posEntryMode'].fillna(df1['posEntryMode'].mean())

df1['posConditionCode'].head(10)

df1['posConditionCode']=df1['posConditionCode'].fillna(df1['posConditionCode'].mean())

df1['transactionType'].head()
df1['transactionType']=df1['transactionType'].fillna('unknown')

# df1['echoBuffer']=df1['echoBuffer'].fillna(df1['echoBuffer'].mean())
df1['echoBuffer'].head()
df1.drop(['echoBuffer','merchantCity','merchantState','merchantZip'	,'posOnPremises','recurringAuthInd'],axis=1,inplace=True)

"""**now the data is cleaned and null values are removed**"""

df1.info()
df1.isnull().sum()

"""**checking data patterns for fraud=true**"""

fraud_data = df1[df1['isFraud'] == True]  # Filter for isFraud == True
# null_in_fraud_rows = fraud_data.isnull().any(axis=1)  # Check for nulls in those rows
# print(null_in_fraud_rows.any())
print(fraud_data.head(10))
print(fraud_data.info())

import pandas as pd

# Set display options to show all columns
pd.set_option('display.max_columns', None)

# Filter and Print
fraud_data = df1[df1['isFraud'] == True]
print(fraud_data.head(5))

"""**converting date**"""

from datetime import datetime

df1['transactionDateTime'] = pd.to_datetime(df1['transactionDateTime'])

df1['transaction_year'] = df1['transactionDateTime'].dt.year
df1['transaction_month'] = df1['transactionDateTime'].dt.month
df1['transaction_day'] = df1['transactionDateTime'].dt.day
df1['transaction_hour'] = df1['transactionDateTime'].dt.hour
df1['transaction_minute'] = df1['transactionDateTime'].dt.minute
df1['transaction_second'] = df1['transactionDateTime'].dt.second

import pandas as pd

# Set display options to show all columns
pd.set_option('display.max_columns', None)

# Filter and Print
fraud_data = df1[df1['isFraud'] == True]
print(fraud_data.head(5))

df1['transaction_time']=df1['transaction_hour']*60*60+df1['transaction_minute']*60+df1['transaction_second']
df1['transaction_time'].head()

"""# **checking the importance of features in our data **"""

import seaborn as sns
sns.boxplot(x='isFraud', y='transactionAmount', data=df1)
sns.countplot(x='isFraud', hue='cardPresent', data=df1)

df1['cardPresent'].value_counts()

sns.boxplot(x='isFraud',y='posEntryMode',data=df1)
sns.countplot(x='isFraud',hue='posConditionCode',data=df1)

df1.info()

"""**droping columns and creating new features**"""

df1.drop(columns=['accountNumber'],axis=1,inplace=True)
df1.drop(columns=['merchantName'],axis=1,inplace=True)
df1.drop(columns=['transactionDateTime','transaction_year','transaction_month','transaction_day','transaction_hour','transaction_minute','transaction_second'],axis=1,inplace=True)
df1.drop(columns=['posEntryMode','posConditionCode','cardCVV','enteredCVV','cardLast4Digits'],axis=1,inplace=True)
# df1.drop(columns=['amountToLimitRatio','avgTransactionAmount'],axis=1,inplace=True)
# df1['amountToLimitRatio'] = df1['transactionAmount'] / df1['creditLimit']
# df1['avgTransactionAmount'] = df1.groupby('accountNumber')['transactionAmount'].transform(lambda x: x.rolling(7).mean())
df1.drop(columns=['expirationDateKeyInMatch','cardPresent'],axis=1,inplace=True)


# Creating new features
df1["UtilizationRate"] = (df1["currentBalance"] + df1["transactionAmount"]) / df1["creditLimit"] * 100

df1["RemainingCredit"] = df1["creditLimit"] - (df1["currentBalance"] + df1["transactionAmount"])

df1["TransactionToBalanceRatio"] = df1["transactionAmount"] / (df1["currentBalance"] + 1e-5)


df1["TransactionFrequency"] = df1["transaction_time"].diff().fillna(0)

# print(df1)
# Set display options to show all columns
pd.set_option('display.max_columns', None)

# Filter and Print
fraud_data = df1[df1['isFraud'] == True]
print(fraud_data.head(5))
# Set display options to show all columns
pd.set_option('display.max_columns', None)

sns.barplot(x='isFraud',y='transactionType',data=df1)
# sns.barplot(x='isFraud',y='',data=df1)

df1.drop(columns=['creditLimit' , 'availableMoney' , 'transactionAmount'  ,'currentBalance'],axis=1,inplace=True)
df1.drop(columns=['Unnamed: 0','currentExpDate'],axis=1,inplace=True)
df1.info()

# from datetime import date
# # Convert to datetime
# df1["accountOpenDate"] = pd.to_datetime(df1["accountOpenDate"])
# df1["dateOfLastAddressChange"] = pd.to_datetime(df1["dateOfLastAddressChange"])

# # Remove time component
# df1["DaysSinceOpenAcnt"] = (df1["accountOpenDate"].dt.floor('D') - df1["dateOfLastAddressChange"].dt.floor('D')).dt.days


df1.drop(columns=['accountOpenDate','dateOfLastAddressChange'],axis=1,inplace=True)
# df1.drop(columns=['DaysSinceOpenAcnt'],axis=1,inplace=True)

"""**now all numerical values are converted and ready**"""

df1.head()

"""**one hot encoding for categorical values**"""

dummy= pd.get_dummies(df1, columns=['acqCountry', 'merchantCountryCode','merchantCategoryCode','transactionType'], drop_first=True)

print(dummy.head())

"""**giving values to x and y**"""

x=dummy.drop(columns=['isFraud'])
x

y=df1['isFraud']
y

"""**install lirary to balance data**"""

pip install imbalanced-learn

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

"""**dividing into train and test**"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

"""**applying smote**"""

smote = SMOTE(random_state=42)
x_train_res, y_train_res = smote.fit_resample(x_train, y_train)

"""**cheking after smote **"""

print("Before SMOTE:")
print(y_train.value_counts())
# print(x_train.value_counts())

print("After SMOTE:")
print((y_train_res).value_counts())
# print((x_train_res).value_counts())

"""# **fit the model ,train it **"""

scaler = StandardScaler()
x_train_res_scaled = scaler.fit_transform(x_train_res)
x_test_scaled = scaler.transform(x_test)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# model
model = LogisticRegression()

# Train
model.fit(x_train_res_scaled, y_train_res)

len(y_train_res)
y_train_res.value_counts()

"""**predictions and check accuracy**"""

# predict
y_pred = model.predict(x_test_scaled)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))



"""**random forest model**"""

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(class_weight='balanced')
model.fit(x_train_res, y_train_res)

model.score(x_train_res,y_train_res)

model.score(x_test_scaled,y_test)

len(x_test_scaled)

len(y_test)

y_test.value_counts()

y_pred=model.predict(x_test_scaled)
print("Accuracy:",accuracy_score(y_test,y_pred))
print("Classification:\n",classification_report(y_test,y_pred))

import numpy as np
unique, counts = np.unique(y_train_res, return_counts=True)
print("Resampled class distribution:", dict(zip(unique, counts)))

"""**xgboost algorithm**"""

import xgboost as xgb
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score

dtrain = xgb.DMatrix(x_train_res, label=y_train_res)
dtest = xgb.DMatrix(x_test, label=y_test)

params = {
    'objective': 'binary:logistic',
    'eval_metric': 'logloss',
    'learning_rate': 0.05,
    'max_depth': 6,
}

model_xgb = xgb.train(params, dtrain, num_boost_round=100, evals=[(dtest, "Test")], early_stopping_rounds=10)

y_pred_proba_xgb = model_xgb.predict(dtest)
y_pred_xgb = (y_pred_proba_xgb >= 0.5).astype(int)


print("XGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("XGBoost Classification Report:\n", classification_report(y_test, y_pred_xgb))
print("XGBoost AUC-ROC:", roc_auc_score(y_test, y_pred_proba_xgb))

"""**lightgbm algorithm**"""

import lightgbm as lgb
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score


lgb_train = lgb.Dataset(x_train_res, label=y_train_res)
lgb_test = lgb.Dataset(x_test, label=y_test, reference=lgb_train)


params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'boosting_type': 'gbdt',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.8,
    # 'is_unbalance': True
}


model_lgb = lgb.train(params, lgb_train, num_boost_round=100, valid_sets=[lgb_train, lgb_test])


y_pred_proba_lgb = model_lgb.predict(x_test)
y_pred_lgb = (y_pred_proba_lgb >= 0.5).astype(int)


print("LightGBM Accuracy:", accuracy_score(y_test, y_pred_lgb))
print("LightGBM Classification Report:\n", classification_report(y_test, y_pred_lgb))
print("LightGBM AUC-ROC:", roc_auc_score(y_test, y_pred_proba_lgb))

model.score(x_test_scaled,y_test)

model.score(x_train_res_scaled,y_train_res)